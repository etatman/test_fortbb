# -*- coding: utf-8 -*-
"""Bitcoin ML APP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OmqjhNX2zvPai1oDYx-jtod5_GBlDyQP

This ML App takes historical Bitcoin data to predict if you should buy or sell shares of Bitcoin. This App provides you with five graphs that support it's decision to buy or sell. The first graph shows the raw data in four lines one for Open, High, Low, and Close. The second shows the rata data after it has been reshaped through a log function. The third graph shows the data that the model was trained on in blue, the applicaiton's predictions in black, and the test data in red for the price of Bitcoin at Close. The Fourth graph is a focused display of the previous graph at were the test and prediction data split. The fifth graph places the applications prediction line on top of the raw data.

Currently, while the actual coin outpreformed the model's prediction, the applicaiton still reccomended buying more Bitcoin which would have been the best move at that time. This app could be imporved by furthur breaking the model down into smaller time differentials such as quarters or weeks to give the user more information.

This data is taken from coinmarketcap.
"""

#from google.colab import files
#uploaded = files.upload()

import pandas as pd
import io

url = 'https://raw.github.iu.edu/etatman/E222/master/bitcoin_price_1week_Test%20-%20Test.csv?token=AAADWLB7O5KRSXZ4QZPVI7LAKDLPK'
B_test = pd.read_csv(url)

url = 'https://raw.github.iu.edu/etatman/E222/master/bitcoin_price_Training%20-%20Training.csv?token=AAADWLFOM4DFILM27BP7PR3AKDLRI'
B_train = pd.read_csv(url)

#

#from google.colab import files
#uploaded = files.upload()

#B_test = pd.read_csv(io.BytesIO(uploaded['bitcoin_price_1week_Test - Test.csv']))

#B_test

B_test['Date'] = pd.to_datetime(B_test['Date'])
#B_test

B_train['Date'] = pd.to_datetime(B_train['Date'])

#B_train.shape
#B_train

#B_test.shape

#B_train.describe()

#B_test.describe()

import matplotlib.pyplot as plt

#B_train.plot(x='Open', y='Close', style='x')  
#plt.title('Open vs Close')  
#plt.xlabel('Open')  
#plt.ylabel('Close')  
#plt.show()

import seaborn as sb
#sb.distplot(B_train['Open'],kde=True)

#sb.distplot(B_train['Close'],kde=True)

#B_train.size

#B_train
#x_int = x.is_integer()
#print(x)
#Open = B_train['Open']
#Open = int(Open)

from sklearn.metrics import mean_squared_error, r2_score

x_tr = B_train['Open'].values.reshape(-1,1)
y_tr = B_train['Close'].values.reshape(-1,1)
x_ts = B_test['Open'].values.reshape(-1,1)
y_ts = B_test['Close'].values.reshape(-1,1)
#from sklearn.model_selection import train_test_split
#split = 0.2 ; 

#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=split, random_state=0)

from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
my_mdl = lin_reg.fit(x_tr,y_tr)

#intercept
#print(my_mdl.intercept_)
#slope
#print(my_mdl.coef_)

my_pred = lin_reg.predict(x_ts)

testdf = pd.DataFrame({'Actual': y_ts.flatten(), 'Predicted': my_pred.flatten()})
#testdf

import numpy as np

Y_predict = np.zeros( len(y_ts) ); 
n = 0; 
for x in x_ts:
    Y_predict[n] = lin_reg.predict( [x] ); 
    n += 1; 
    

y_min = np.min( y_ts ); 
y_max = np.max( y_ts ); 

#plt.figure(Linear Regression) 
#plt.plot( Y_predict, y_ts , 'bo' );
  
#plt.plot( [y_min,y_max],[y_min,y_max],'r'); 
   
#plt.xlabel('Predicted value',fontsize=20)
#plt.ylabel('Actual value',fontsize=20)
#plt.axis('image')

from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y_ts, Y_predict)
#print("Mean Absolute Error: ", mae)

mse = mean_squared_error(y_ts, Y_predict)
#print('Mean Squared Error: ', mse)

rmse = np.sqrt(mean_squared_error(y_ts, Y_predict))
#print('Root Mean Squared Error: ', rmse)

r2 = r2_score(y_ts, Y_predict)
#print('R2: ', r2)

B_train_flip = B_train.sort_index(axis=1 ,ascending=True)
B_train_flip2 = B_train_flip.iloc[::-1]
#B_train_flip = B_train[columns]
#B_train_flip2 #had to flip the data from oldest to newest to predict future and not past

plt.rcParams['figure.figsize'] = (25,25) 
plt.rcParams['lines.markersize'] = 5; 
T = np.linspace(0, 1556, 1556	)
plt.figure()# Raw Data
plt.plot( T , B_train_flip2['Open'] , 'ro' , linestyle='solid')
plt.plot( T, B_train_flip2['High'], 'bo' , linestyle='solid')
plt.plot(T, B_train_flip2['Low'], 'go', linestyle='solid')
plt.plot( T, B_train_flip2['Close'], 'ko' , linestyle='solid')
plt.xlabel( 'Time (days)' , fontsize=25)
plt.ylabel( 'Dollars (USD)' , fontsize=25)

logP1 = np.log(B_train_flip2['Open']);
logP2 = np.log(B_train_flip2['High']);
logP3 = np.log(B_train_flip2['Low']); 
logP4 = np.log(B_train_flip2['Close'])

plt.figure()#Logistic Regression NOT Trained
plt.plot( T , logP1 , 'ro' , linestyle='solid') ; 
plt.plot( T, logP2, 'bo' , linestyle='solid'); 
plt.plot( T, logP3, 'ko' , linestyle='solid') ;
plt.plot(T, logP4,'go', linestyle='solid')
plt.xlabel( 'Time (days)' , fontsize=25); 
plt.ylabel( 'Dollars (log scale)' , fontsize=25);

N = len(T);
N_train = 1245
T_train = T[0:N_train+1]
logP1_train = logP1[0:N_train+1];
logP2_train = logP2[0:N_train+1];
logP3_train = logP3[0:N_train+1];
logP4_train = logP4[0:N_train+1]

T_test = T[N_train+1:N];
logP1_test = logP1[N_train+1:N];
logP2_test = logP2[N_train+1:N];
logP3_test = logP3[N_train+1:N];
logP4_test = logP4[N_train+1:N]

# import key sklearn functions 
from sklearn import linear_model

regr1 = linear_model.LinearRegression(); 
regr1.fit( T_train.reshape(-1,1) , logP1_train ); 
predictions1 = regr1.predict( T_test.reshape(-1,1) ); 

regr2 = linear_model.LinearRegression(); 
regr2.fit( T_train.reshape(-1,1) , logP2_train ); 
predictions2 = regr2.predict( T_test.reshape(-1,1) ); 

regr3 = linear_model.LinearRegression(); 
regr3.fit( T_train.reshape(-1,1) , logP3_train ); 
predictions3 = regr3.predict( T_test.reshape(-1,1) ); 

regr4 = linear_model.LinearRegression(); 
regr4.fit( T_train.reshape(-1,1) , logP4_train ); 
predictions4 = regr4.predict( T_test.reshape(-1,1) );

#plt.figure(4)
#plt.plot( T_train, logP1_train, 'bo', markersize=15); 
#plt.plot( T_test, logP1_test ,'ro' , markersize=15 ) ; 
#plt.plot( T_test , predictions1, 'kP' , markersize=15 ) 
#plt.title( 'Open' ); plt.legend( ['train', 'test', 'predictions']); plt.show()


#plt.figure(5)
#plt.plot( T_train, logP2_train, 'bo' , markersize=15 ); 
#plt.plot( T_test, logP2_test ,'ro' , markersize=15 ) ; 
#plt.plot( T_test , predictions2, 'kP' , markersize=15 ) 
#plt.title( 'High' ); plt.legend( ['train', 'test', 'predictions']); plt.show()


#plt.figure(6)
#plt.plot( T_train, logP3_train, 'bo' , markersize=15 ); 
#plt.plot( T_test, logP3_test ,'ro' , markersize=15 ) ; 
#plt.plot( T_test , predictions3, 'kP' , markersize=15 ) 
#plt.title( 'Low' );plt.legend( ['train', 'test', 'predictions']); plt.show()


plt.figure()#Logistic Regression Model
plt.plot( T_train, logP4_train, 'bo' , markersize=15 ); 
plt.plot( T_test, logP4_test ,'ro' , markersize=15 ) ; 
plt.plot( T_test , predictions4, 'kP' , markersize=15 ) 
plt.title( 'Close' );plt.legend( ['train', 'test', 'predictions']); plt.show()

#set = np.concatenate( (logP1_test,predictions1) ); 
#m1 = min(set); m2 = max(set) ; 
#plt.figure(7); plt.clf()
#plt.plot(predictions1, logP1_test , 'ro' , markersize=15); 
#plt.plot( [m1,m2],[m1,m2], 'k', linewidth=5);
#plt.ylabel('Actual'); plt.xlabel('Predictions'); plt.show()


#plt.figure(8); plt.clf(); 
#set = np.concatenate( (logP2_test,predictions2) ); 
#m1 = min(set); m2 = max(set) ; 
#plt.plot(predictions2, logP2_test, 'ro' , markersize=15); 
#plt.plot( [m1,m2],[m1,m2], 'k', linewidth=5);
#plt.ylabel('Actual'); plt.xlabel('Predictions'); plt.show()


#plt.figure(9); plt.clf()
#set = np.concatenate( (logP3_test,predictions3) ); 
#m1 = min(set); m2 = max(set) ; 
#plt.plot( predictions3, logP3_test, 'ro' , markersize=15) ; 
#plt.plot( [m1,m2],[m1,m2], 'k', linewidth=5);
#plt.ylabel('Actual'); plt.xlabel('Predictions'); plt.show()


plt.figure() #Focused View of Prediction v. Test Data Split
plt.clf()
set = np.concatenate( (logP4_test,predictions4) ); 
m1 = min(set); m2 = max(set) ; 
plt.plot( predictions4, logP4_test, 'ro' , markersize=15) ; 
plt.plot( [m1,m2],[m1,m2], 'k', linewidth=5);
plt.ylabel('Actual'); plt.xlabel('Predictions'); plt.show()

from sklearn.metrics import r2_score 

p1 = r2_score( logP1_test , predictions1 )
#print( p1 )


p2 = r2_score( logP2_test , predictions2 ) 
#print( p2 )


p3 = r2_score( logP3_test , predictions3 ) 
#print( p3 )

p4 = r2_score( logP4_test , predictions4 ) 
#print( p4 )

#plt.rcParams['lines.markersize'] = 15 
#plt.figure(10); plt.clf(); 
#plt.plot( T_train, np.exp(logP1_train), 'bo' ); 
#plt.plot( T_test, np.exp(logP1_test) ,'ro' ) ; 
#plt.plot( T_test , np.exp(predictions1), 'kP' ) 
#plt.title( 'Open' ); plt.legend(['train','test','predicted']); plt.show()


#plt.figure(11); plt.clf(); 
#plt.plot( T_train, np.exp(logP2_train), 'bo' ); 
#plt.plot( T_test, np.exp(logP2_test),'ro' ) ; 
#plt.plot( T_test , np.exp(predictions2), 'kP' ) 
#plt.title( 'High' ); plt.legend(['train','test','predicted']);plt.show();


#plt.figure(12); plt.clf(); 
#plt.plot( T_train, np.exp(logP3_train), 'bo' ); 
#plt.plot( T_test, np.exp(logP3_test),'ro' ) ; 
#plt.plot( T_test , np.exp(predictions3), 'kP' ) 
#plt.title( 'Low' ); plt.legend(['train','test','predicted']);  plt.show();


plt.figure()#Predictions Compared to Actual Data
plt.clf(); 
plt.plot( T_train, np.exp(logP4_train), 'bo' ); 
plt.plot( T_test, np.exp(logP4_test),'ro' ) ; 
plt.plot( T_test , np.exp(predictions4), 'kP' ) 
plt.title( 'Close' ); plt.legend(['train','test','predicted']);  plt.show();

slope, intercept = np.polyfit(np.log(T_test), np.log(predictions4), 1)
#print(slope)

def output():
  if slope <= 0:
    print("Sell")
  else:
    print("Buy")

return 

my_model = regr4.fit( T_train.reshape(-1,1) , logP4_train );
from joblib import dump, load 
dump(my_model, 'Bitcoin_model.pkl')
